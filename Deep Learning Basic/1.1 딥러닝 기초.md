# 1.1 딥러닝 기본 용어 설명

### Introduction

Implementation Skills  - 구현 능력 필요

Math Skills - 선형대수, 확률론이 중요

Knowing a lot of recent Papers 

___

### Artificial Inteligence의 정의 

사람의 지능을 모방하는 것

#### Machine Learning 

데이터를 통해 학습하는 것

#### Deep Learning

데이터를 통한 기계학습 중 가장 안쪽의 기술

___

### Key Components of Deep Learning 

- The **data** that the model can learn from
- The **model** how to transform the data
- The **loss** function that quatifies the badness of the model
- The **algorithm** to adjust the parameters to minimize the loss

___

## Data

- Classification
  - 강아지와 고양이 분류
- Semantic Segmentation
  - 각 이미지의 픽셀별로 분류
- Detection
  - 이미지 안 물체의 영역 박스 출력
- Pose Estimation
  - 이미지 안 물체의 스켈레톤 정보
- Visual QnA
  - 이미지와 질문이 주어졌을 때 답을 구하는 것

## Model 

- AlexNet
- GoogLeNet
- DenseNet
- LSTM
- GAN

등등..

## Loss

The loss function is a proxy of what we want to achieve

데이터와 모델이 있을 때 이를 어떻게 잘 학습할지

문제를 풀 때 단순히 loss 값을 줄이는 것이 목적은 아님

- Regression Task = MSE
- Classification Task = CE
- Probabilistic Task = MLE

## Optimization Algorithm 최적화 방법
 
보통 뉴럴 네트워크의 파라미터를 로스에 대하여 1차 미분한 정보를 활용

모델이 학습하지 않은 데이터(실환경)에서 잘 동작하게 하는 것이 목적

다양한 테크닉을 같이 활용해야 함


# 1.2 Historical Review

## 2012 - AlexNet

224 * 224 이미지 분류가 목적

역사적으로 딥러닝이 실제 성능을 발휘하기 시작함

## 2013 - DQN

알파고를 만든 방법론

## 2014 - Encoder / Decoder

다른 언어의 문장이 주어졌을 때 원하는 다른 언어의 문장으로 뱉어주는 것

## 2014 - Adam Optimizer

결과가 잘 나옴 

## 2015 - Generative Adverwarial Network

중요!

~~술집이름이 논문에 들어감~~

## 2015 - Residual Networks

딥러닝이 딥러닝이 가능해진 논문

뉴럴 네트워크를 100단 이상 쌓아도 성능이 잘 나오게 만들었음

## 2017 - Transformer

중요!

## 2018 - BERT (fine-tuned NLP models)

자연어 처리 문제는 랭귀지 모델을 학습함

랭귀지 모델은 이전의 단어들로 다음 단어를 맞추게 함

다양한 단어, 큰 말뭉치를 파인 튜닝하여 좋은 성능을 내게 함

## 2019 - BIG Language Models

1750억개로 굉장히 많은 파라미터로 되어 있는 것이 특징

## 2020 - Self Supervised Learning

이미지 분류와 같은 분류 문제에서 한정된 학습데이터 외에 라벨을 모르는 데이터도 활용하는 것

강아지와 고양이 라벨 데이터와 라벨이 없는 다양한 이미지들을 학습에 같이 활용